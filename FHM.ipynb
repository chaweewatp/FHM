{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สิ่งที่ต้องเตรียมก่อน\n",
    "ไฟล์จาก OMS report >> http://172.30.7.213/omsreport >> รายงาน >> กฟน.1 >> รายงานเหตุการณ์ไฟฟ้าขัดข้อง >> รายงานเหตุการไฟฟ้าขัดข้อง 2 เดือนล่าสุด \n",
    "save ข้อมูลลงไฟล์ OMS_N1_1, OMS_N1_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "import neurolab as nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(input):\n",
    "    if input == 1:\n",
    "        output = 9\n",
    "    elif input == 2:\n",
    "        output = 7\n",
    "    elif input == 3:\n",
    "        output = 5\n",
    "    elif input == 4:\n",
    "        output = 3\n",
    "    elif input == 5:\n",
    "        output = 1\n",
    "    elif input == 6:\n",
    "        output = 1/3\n",
    "    elif input == 7:\n",
    "        output = 1/5\n",
    "    elif input == 8:\n",
    "        output = 1/7\n",
    "    else:\n",
    "        output = 1/9\n",
    "    return output\n",
    "\n",
    "def count_APSA(df, feeder):\n",
    "    temp_df=df.drop(set(np.where(df['feedername']!=feeder+'VB01')[0])).copy()\n",
    "    print(temp_df)\n",
    "    if temp_df['total'].vals==0:\n",
    "        return {'percent_complete':0}\n",
    "    else:\n",
    "        return {'percent_complete':1-(temp_df['complete']/temp_df['total'])}\n",
    "\n",
    "\n",
    "def count_counter(df, feeder):\n",
    "    temp_df=df.drop(set(np.where(df['ฟีดเดอร์']!=feeder)[0])).copy()\n",
    "#     print(temp_df)\n",
    "    return {'T/R':list(temp_df['ประเภทการทำงาน']).count('T/R'), 'T/L':list(temp_df['ประเภทการทำงาน']).count('T/L')}\n",
    "\n",
    "def nor_input_layer(df):\n",
    "    df['sum_TR']=[item/10   if item < 10 else 1 for item in df['sum_TR']]\n",
    "    df['sum_TL']=[item/10   if item < 10 else 1 for item in df['sum_TL']]\n",
    "    df['Peak load']=[item/10   for item in df['Peak load']]\n",
    "    df['Average load']=[item/10   for item in df['Average load']]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_max_load_N1(area, feeder, year,month):\n",
    "    if area == 1:\n",
    "        url = \"http://172.30.7.209/services/get_load.php\"\n",
    "\n",
    "        querystring = {\"region\":\"11\",\"year\":\"{}\".format(year),\"month\":\"{}\".format(month),\"substaion\":\"{}\".format(feeder[:3]),\"feeder\":\"OUT{}\".format(feeder[3:])}\n",
    "\n",
    "\n",
    "        headers = {\n",
    "            'Cache-Control': \"no-cache\",\n",
    "#             'Postman-Token': \"eecdd298-920a-4726-91ba-f3f8d48ef2a8\"\n",
    "            }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        response=response.json()\n",
    "        if response['data'] != []:\n",
    "            return float(response['max'])\n",
    "        else:\n",
    "            return float(0)\n",
    "\n",
    "\n",
    "def get_ava_load_N1(area, feeder, year,month):\n",
    "    if area == 1:\n",
    "        url = \"http://172.30.7.209/services/get_load.php\"\n",
    "\n",
    "        querystring = {\"region\":\"11\",\"year\":\"{}\".format(year),\"month\":\"{}\".format(month),\"substaion\":\"{}\".format(feeder[:3]),\"feeder\":\"OUT{}\".format(feeder[3:])}\n",
    "\n",
    "        headers = {\n",
    "            'Cache-Control': \"no-cache\",\n",
    "#             'Postman-Token': \"eecdd298-920a-4726-91ba-f3f8d48ef2a8\"\n",
    "            }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        response=response.json()\n",
    "\n",
    "        if response['data'] != []:\n",
    "            return float(response['avg'])\n",
    "        else:\n",
    "            return float(0)\n",
    "def get_max_load_N2(area, feeder, year,month):\n",
    "    if area == 2:\n",
    "        url = \"http://172.30.7.209/services/get_load.php\"\n",
    "\n",
    "        querystring = {\"region\":\"12\",\"year\":\"{}\".format(year),\"month\":\"{}\".format(month),\"substaion\":\"{}\".format(feeder[:3]),\"feeder\":\"OUT{}\".format(feeder[3:])}\n",
    "\n",
    "\n",
    "        headers = {\n",
    "            'Cache-Control': \"no-cache\",\n",
    "#             'Postman-Token': \"eecdd298-920a-4726-91ba-f3f8d48ef2a8\"\n",
    "            }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        response=response.json()\n",
    "        if response['data'] != []:\n",
    "            return float(response['max'])\n",
    "        else:\n",
    "            return float(0)\n",
    "\n",
    "\n",
    "def get_ava_load_N2(area, feeder, year,month):\n",
    "    if area == 2:\n",
    "        url = \"http://172.30.7.209/services/get_load.php\"\n",
    "\n",
    "        querystring = {\"region\":\"12\",\"year\":\"{}\".format(year),\"month\":\"{}\".format(month),\"substaion\":\"{}\".format(feeder[:3]),\"feeder\":\"OUT{}\".format(feeder[3:])}\n",
    "\n",
    "        headers = {\n",
    "            'Cache-Control': \"no-cache\",\n",
    "#             'Postman-Token': \"eecdd298-920a-4726-91ba-f3f8d48ef2a8\"\n",
    "            }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "        response=response.json()\n",
    "\n",
    "        if response['data'] != []:\n",
    "            return float(response['avg'])\n",
    "        else:\n",
    "            return float(0)\n",
    "\n",
    "def get_peak_load_N3(area, feeder, year, month):\n",
    "    if area == 3:\n",
    "#         print(feeder)\n",
    "        url = \"http://172.30.200.113/webcenter/views/peakload.php\"\n",
    "        querystring = {\"year\":\"{}\".format(year),\"month\":\"{}\".format(month),\"region\":\"13\", 'feedername':'{}'.format(feeder)+'VB01'}\n",
    "        headers = {\n",
    "        'Cache-Control': \"no-cache\",\n",
    "        #     'Postman-Token': \"15f82df5-f82f-4596-bc0b-efd65dd23b28\"\n",
    "        }\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "        response=response.json()\n",
    "        if response['PeakLoad'] == []:\n",
    "            return float(0)\n",
    "        else:\n",
    "            return float(response['PeakLoad'][0]['MW'])\n",
    "        \n",
    "def clean_feeder(feeder):\n",
    "    correct_feeder = ['OUT01', 'OUT02','OUT03','OUT04','OUT05','OUT06','OUT07','OUT08','OUT09','OUT10', 'OUT11','OUT12']\n",
    "    if feeder not in correct_feeder:\n",
    "        if feeder == 'OUT010':\n",
    "            return 'OUT10'\n",
    "        else:\n",
    "            return 'OUT0'+feeder[3:]\n",
    "    else:\n",
    "        return feeder\n",
    "    \n",
    "def get_max_avarage_load(df, feeder):\n",
    "    temp_df=df.drop(np.where(df['feeder']!=feeder)[0]).copy()\n",
    "    return abs(temp_df['WERT_MAX'].max()), abs(temp_df['WERT_EFF'].mean())\n",
    "\n",
    "def get_TL_TR_values(df1, df2, List_feeder_name):\n",
    "    \n",
    "    df1=df1.drop(set(np.where(df1['ประเภทการทำงาน']=='D/F')[0]).union(set(np.where(df1['ประเภทการทำงาน']=='Operate')[0])))\n",
    "    df1=df1.reset_index()\n",
    "    del df1['index']\n",
    "    del df1['หมายเลขเหตุการณ์']\n",
    "    del df1['ลำดับ']\n",
    "    del df1['วันที่/เวลาไฟดับ']\n",
    "    del df1['เวลา']\n",
    "    del df1['วันที่/เวลา ที่จ่ายไฟคืนระบบได้ครั้งแรก']\n",
    "    del df1['วันที่/เวลา ที่จ่ายไฟคืนครบทั้งหมด']\n",
    "    del df1['รวมเวลาไฟดับ (นาที)']\n",
    "    del df1['เฟส']\n",
    "    del df1['สาเหตุ/รายละเอียด']\n",
    "    del df1['ทราบสาเหตุ']\n",
    "    del df1['กฟฟ.รับผิดชอบ']\n",
    "    del df1['สภาพอากาศ']\n",
    "    del df1['ผชฟ. ถูกกระทบ (ราย)']\n",
    "    del df1['สถานที่จุดเกิดเหตุ']\n",
    "    del df1['รายละเอียดการแก้ไข']\n",
    "    del df1['ค่าโหลด (MW)']\n",
    "    del df1['ประเภทเหตุการณ์']\n",
    "\n",
    "    df2=df2.drop(set(np.where(df2['ประเภทการทำงาน']=='D/F')[0]).union(set(np.where(df2['ประเภทการทำงาน']=='Operate')[0])))\n",
    "    df2=df2.reset_index()\n",
    "    del df2['index']\n",
    "    del df2['หมายเลขเหตุการณ์']\n",
    "    del df2['ลำดับ']\n",
    "    del df2['วันที่/เวลาไฟดับ']\n",
    "    del df2['เวลา']\n",
    "    del df2['วันที่/เวลา ที่จ่ายไฟคืนระบบได้ครั้งแรก']\n",
    "    del df2['วันที่/เวลา ที่จ่ายไฟคืนครบทั้งหมด']\n",
    "    del df2['รวมเวลาไฟดับ (นาที)']\n",
    "    del df2['เฟส']\n",
    "    del df2['สาเหตุ/รายละเอียด']\n",
    "    del df2['ทราบสาเหตุ']\n",
    "    del df2['กฟฟ.รับผิดชอบ']\n",
    "    del df2['สภาพอากาศ']\n",
    "    del df2['ผชฟ. ถูกกระทบ (ราย)']\n",
    "    del df2['สถานที่จุดเกิดเหตุ']\n",
    "    del df2['รายละเอียดการแก้ไข']\n",
    "    del df2['ค่าโหลด (MW)']\n",
    "    del df2['ประเภทเหตุการณ์']\n",
    "    df3=df1.append(df2)\n",
    "    df3=df3.reset_index()\n",
    "    del df3['index']\n",
    "    return {feeder:count_counter(df3, feeder) for feeder in List_feeder_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "            np.int16, np.int32, np.int64, np.uint8,\n",
    "            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
    "            np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = 2018\n",
    "month = 12\n",
    "region = 1 #N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_Criteria=10\n",
    "dict_RI={1:0, 2:0, 3:0.58, 4:0.9, 5:1.12, 6:1.24, 7:1.32, 8:1.41, 9:1.46, 10:1.5}\n",
    "data=pd.read_csv('surveys.csv')\n",
    "data=data.dropna()\n",
    "data=data.reset_index()\n",
    "df3=pd.DataFrame()\n",
    "weights_list=['w1','w2','w3','w4','w5','w6','w7','w8','w9','w10']\n",
    "df3=pd.DataFrame(columns=weights_list)\n",
    "list(data.index)\n",
    "CR_val=[]\n",
    "dict={}\n",
    "for index in list(data.index):\n",
    "    input_file=list(data.iloc[index,:])\n",
    "    prepared_file=[]\n",
    "    for x in input_file:\n",
    "        prepared_file.append(decode(x))\n",
    "    columns_name=['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10']\n",
    "    #columns_name =['CB', 'RCS', 'Load Break', 'LRC', 'Recloser', 'SCB', 'SCB(FRTU)', 'FCB', 'LRR', 'AVR']\n",
    "    #w1, w2, w4, w7\n",
    "    df = pd.DataFrame(np.zeros([number_Criteria,number_Criteria]), columns=columns_name, index=columns_name)\n",
    "    for name in columns_name:\n",
    "        df[name][name]=1\n",
    "\n",
    "        mini_list1=np.array(prepared_file[0:9].copy())  #9-0 =9\n",
    "    df.iloc[0,1:10]=mini_list1\n",
    "    df.iloc[1:10,0]=1/mini_list1\n",
    "\n",
    "    mini_list2 = np.array(prepared_file[9:17].copy())   # 17-9 =8\n",
    "    df.iloc[1,2:10]=mini_list2\n",
    "    df.iloc[2:10,1]=1/mini_list2\n",
    "\n",
    "    mini_list3 = np.array(prepared_file[17:24].copy())   #.  24-17 = 7\n",
    "    df.iloc[2,3:10]=mini_list3\n",
    "    df.iloc[3:10,2]=1/mini_list3\n",
    "\n",
    "    mini_list4 = np.array(prepared_file[24:30].copy())   #.  30-24 = 6\n",
    "    df.iloc[3,4:10]=mini_list4\n",
    "    df.iloc[4:10,3]=1/mini_list4\n",
    "\n",
    "    mini_list5 = np.array(prepared_file[30:35].copy())   #.  35-30 = 5\n",
    "    df.iloc[4,5:10]=mini_list5\n",
    "    df.iloc[5:10,4]=1/mini_list5\n",
    "\n",
    "    mini_list6 = np.array(prepared_file[35:39].copy())   #.  39-35 = 4\n",
    "    df.iloc[5,6:10]=mini_list6\n",
    "    df.iloc[6:10,5]=1/mini_list6\n",
    "\n",
    "    mini_list7 = np.array(prepared_file[39:42].copy())   #.  42-39 = 3\n",
    "    df.iloc[6,7:10]=mini_list7\n",
    "    df.iloc[7:10,6]=1/mini_list7\n",
    "\n",
    "    mini_list8 = np.array(prepared_file[42:44].copy())   #.  44-42 = 2\n",
    "    df.iloc[7,8:10]=mini_list8\n",
    "    df.iloc[8:10,7]=1/mini_list8\n",
    "\n",
    "    mini_list9 = np.array(prepared_file[44:45].copy())   #.  45-44 = 1\n",
    "    df.iloc[8,9:10]=mini_list9\n",
    "    df.iloc[9:10,8]=1/mini_list9\n",
    "    \n",
    "    df.loc['sum'] = df.sum()\n",
    "    df2=pd.DataFrame(columns=columns_name, index=columns_name)\n",
    "    for item in columns_name:\n",
    "        df2.loc[item]=df.loc[item]/df.loc['sum']\n",
    "    df2['total'] = df2[df2.columns].sum(axis=1)\n",
    "    df2['weight'] = df2['total']/df2['total'].sum()\n",
    "    df2.loc['sum'] = df2.sum()\n",
    "    df3.loc[index]=np.array(list(df2['weight'][0:-1]))\n",
    "\n",
    "    CI_list=[np.array(df.iloc[item,:].sum()*np.array(df2.iloc[0:10,11]))/df2.iloc[item,11] for item in np.arange(0,number_Criteria,1)]\n",
    "    df2['CM']=np.array(CI_list+[np.mean(CI_list)])\n",
    "    \n",
    "    CI_val=(df2.iloc[10,12]-df2.iloc[10,10])/(df2.iloc[10,10]-1)\n",
    "    CR_val.append(CI_val/dict_RI[number_Criteria])\n",
    "    dict.update({index:{'weight':list(df2['weight']), 'CR':CI_val/dict_RI[number_Criteria]}})\n",
    "\n",
    "df3['CR_val']=np.array(CR_val)\n",
    "\n",
    "number_obs=56 #manual added\n",
    "dict_weights={weight: np.prod(np.array(df3[weight]))**(1/number_obs) for weight in weights_list}\n",
    "sum=dict_weights['w1']+dict_weights['w2']+dict_weights['w3']+dict_weights['w4']+dict_weights['w5']+dict_weights['w6']+dict_weights['w7']+dict_weights['w8']+dict_weights['w9']+dict_weights['w10']\n",
    "\n",
    "#dict_nor_weights is output weight \n",
    "dict_nor_weights={weight:dict_weights[weight]/sum for weight in weights_list}\n",
    "# print(dict_nor_weights)\n",
    "\n",
    "\n",
    "pattern = '[A-Z]{3}\\d\\d'\n",
    "rex=re.compile(pattern)\n",
    "\n",
    "json1 = json.loads(open('n1.json').read())\n",
    "List_N1_feeder_name= [rex.findall(dict['attributes']['FACILITYID'][0:5]) for dict in json1['features']]\n",
    "List_N1_feeder_name=[a[0] for a in List_N1_feeder_name if a]\n",
    "\n",
    "set_A=set([x for x in List_N1_feeder_name if List_N1_feeder_name.count(x)>1])\n",
    "# print(set_A)\n",
    "List_N1_feeder_name=list(set(List_N1_feeder_name))\n",
    "set_A=set([x for x in List_N1_feeder_name if List_N1_feeder_name.count(x)>1])\n",
    "# print(set_A)\n",
    "\n",
    "List_N1_CB=[item + 'VB01' for item in List_N1_feeder_name]\n",
    "\n",
    "json2 = json.loads(open('n2.json').read())\n",
    "List_N2_feeder_name= [rex.findall(dict['attributes']['FACILITYID'][0:5]) for dict in json2['features']]\n",
    "List_N2_feeder_name=[a[0] for a in List_N2_feeder_name if a]\n",
    "\n",
    "set_A=set([x for x in List_N2_feeder_name if List_N2_feeder_name.count(x)>1])\n",
    "# print(set_A)\n",
    "List_N2_feeder_name=list(set(List_N2_feeder_name))\n",
    "set_A=set([x for x in List_N2_feeder_name if List_N2_feeder_name.count(x)>1])\n",
    "# print(set_A)\n",
    "\n",
    "List_N2_CB=[item + 'VB01' for item in List_N2_feeder_name]\n",
    "\n",
    "json3 = json.loads(open('n3.json').read())\n",
    "List_N3_feeder_name= [rex.findall(dict['attributes']['FACILITYID'][0:5]) for dict in json3['features']]\n",
    "List_N3_feeder_name=[a[0] for a in List_N3_feeder_name if a]\n",
    "set_A=set([x for x in List_N3_feeder_name if List_N3_feeder_name.count(x)>1])\n",
    "# print(set_A)\n",
    "List_N3_feeder_name=list(set(List_N3_feeder_name))\n",
    "set_A=set([x for x in List_N3_feeder_name if List_N3_feeder_name.count(x)>1])\n",
    "# print(set_A)\n",
    "List_N3_CB=[item + 'VB01' for item in List_N3_feeder_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------start area 1-------------\n",
      "-------------retrieve GIS data of area 1-------------\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='gisn1.pea.co.th', port=80): Max retries exceeded with url: /arcgis/rest/services/PEA/MapServer/10/query?where=1%3D1+AND+FEEDERID+%3D+%27MEA07%27&text=&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&returnDistinctValues=false&resultOffset=&resultRecordCount=&f=pjson (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1152be2e8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x1152be2e8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='gisn1.pea.co.th', port=80): Max retries exceeded with url: /arcgis/rest/services/PEA/MapServer/10/query?where=1%3D1+AND+FEEDERID+%3D+%27MEA07%27&text=&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&returnDistinctValues=false&resultOffset=&resultRecordCount=&f=pjson (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1152be2e8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-69de7c070577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtemp_list_FRTU_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtemp_list_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mres1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://gisn{}.pea.co.th/arcgis/rest/services/PEA/MapServer/{}/query?where=1%3D1+AND+FEEDERID+%3D+%27{}%27&text=&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&returnDistinctValues=false&resultOffset=&resultRecordCount=&f=pjson'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequipment_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='gisn1.pea.co.th', port=80): Max retries exceeded with url: /arcgis/rest/services/PEA/MapServer/10/query?where=1%3D1+AND+FEEDERID+%3D+%27MEA07%27&text=&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&returnDistinctValues=false&resultOffset=&resultRecordCount=&f=pjson (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1152be2e8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))"
     ]
    }
   ],
   "source": [
    "for area in [1,2,3]:\n",
    "    print('-------------start area {}-------------'.format(area))\n",
    "    list_of_equipment_code=[10,11,14,16] #SCB, CB, Recloser, Switch\n",
    "    if area == 1:\n",
    "        all_list_feeder_name=List_N1_feeder_name\n",
    "    elif area ==2:\n",
    "         all_list_feeder_name=List_N2_feeder_name\n",
    "    elif area ==3:    \n",
    "        all_list_feeder_name=List_N3_feeder_name\n",
    "\n",
    "    list_feeder_name=[]\n",
    "    list_FRTU_name=[]\n",
    "    list_type=[]\n",
    "    \n",
    "    print('-------------retrieve GIS data of area {}-------------'.format(area))\n",
    "    for equipment_code in list_of_equipment_code:\n",
    "    #     print(equipment_code)\n",
    "        for feeder_name in all_list_feeder_name: \n",
    "    #         print(feeder_name)\n",
    "            temp_list_feeder_name=[]\n",
    "            temp_list_FRTU_name=[]\n",
    "            temp_list_type=[]\n",
    "            res1=requests.get('http://gisn{}.pea.co.th/arcgis/rest/services/PEA/MapServer/{}/query?where=1%3D1+AND+FEEDERID+%3D+%27{}%27&text=&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&maxAllowableOffset=&geometryPrecision=&outSR=&returnIdsOnly=false&returnCountOnly=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&gdbVersion=&returnDistinctValues=false&resultOffset=&resultRecordCount=&f=pjson'.format(area, equipment_code, feeder_name))\n",
    "            res=res1.json()\n",
    "            if len(res['features'])>0:\n",
    "                temp_list_feeder_name=[feeder_name]*len(res['features'])\n",
    "                temp_list_FRTU_name=[dict['attributes']['FACILITYID'] for dict in res['features']]\n",
    "                if equipment_code == 10:\n",
    "                    temp_list_type=['SCB']*len(res['features'])\n",
    "                elif equipment_code == 11:\n",
    "                    temp_list_type=['CB']*len(res['features'])\n",
    "                elif equipment_code == 14:\n",
    "                    temp_list_type=['Recloser']*len(res['features'])\n",
    "                else:\n",
    "                    temp_list_type=['Switch']*len(res['features'])\n",
    "                list_feeder_name=list_feeder_name+temp_list_feeder_name\n",
    "                list_FRTU_name=list_FRTU_name+temp_list_FRTU_name\n",
    "                list_type=list_type+temp_list_type\n",
    "    df_equipment=pd.DataFrame()\n",
    "    df_equipment['feeder']=list_feeder_name\n",
    "    df_equipment['equipment']=list_FRTU_name\n",
    "    df_equipment['type']=list_type\n",
    "    df_equipment['type'].unique()\n",
    "    df_equipment.to_csv('equipment_{}.csv'.format(area), sep=',')\n",
    "    # print(df_equipment)\n",
    "\n",
    "    dict_feeder = {item1:{item2:[df_equipment.iloc[item3, 1] for item3 in set(np.where(df_equipment['type']==item2)[0]).intersection(np.where(df_equipment['feeder']==item1)[0])] for item2 in df_equipment['type'].unique()} for item1 in all_list_feeder_name}\n",
    "\n",
    "    ## retieve data from E-counter\n",
    "    # if area == 1:\n",
    "    #     list_feeder_name=List_N1_feeder_name\n",
    "    # elif area ==2:\n",
    "    #      list_feeder_name=List_N2_feeder_name\n",
    "    # elif area ==3:    \n",
    "    #     list_feeder_name=List_N3_feeder_name\n",
    "\n",
    "    # dict_1={}\n",
    "    # list_TR=[]\n",
    "    # list_TLT=[]\n",
    "    # list_TLI=[]\n",
    "    # list_counter=[]\n",
    "    # for feeder_name in list_feeder_name:\n",
    "    #     res1=requests.get('http://172.30.200.113/webcenter/views/counter_feed_json.php?year=2018&feedername={}'.format(feeder_name))\n",
    "    #     res1=res1.json()\n",
    "    #     sum_TR=0\n",
    "    #     sum_TLT=0\n",
    "    #     sum_TLI=0\n",
    "    #     sum_counter=0\n",
    "    #     for dict in res1:\n",
    "    #         sum_TR=sum_TR+dict['tr']\n",
    "    #         sum_TLT=sum_TLT+dict['tlt']\n",
    "    #         sum_TLI=sum_TLI+dict['tli']\n",
    "    #         sum_counter=sum_counter+dict['sum1']\n",
    "    #     list_TR.append(sum_TR)\n",
    "    #     list_TLT.append(sum_TLT)\n",
    "    #     list_TLI.append(sum_TLI)\n",
    "    #     list_counter.append(sum_counter)\n",
    "    # df_e_counter=pd.DataFrame()\n",
    "    # df_e_counter['feeder']=list_feeder_name\n",
    "    # df_e_counter['sum_TR']=list_TR\n",
    "\n",
    "    # df_e_counter['sum_TLT']=list_TLT\n",
    "    # df_e_counter['sum_TLI']=list_TLI\n",
    "    # df_e_counter['counter']=list_counter\n",
    "    # df_e_counter.to_csv('e_counter_DataFrame_{}.csv'.format(area), sep=',')\n",
    "    # # print(df_e_counter)\n",
    "\n",
    "    #retieve APSA data\n",
    "    print('-------------retrieve APSA data of area {}-------------'.format(area))\n",
    "    res2=requests.get(\"https://region1.pea.co.th/api/apsa/status\")\n",
    "    res2=res2.json()\n",
    "    list_feedername=[dict['feedername'] for dict in res2['records']]\n",
    "    list_finished=[int(dict['finished']) for dict in res2['records']]\n",
    "    list_operation=[int(dict['operation']) for dict in res2['records']]\n",
    "    df_APSA=pd.DataFrame()\n",
    "    df_APSA['feedername']=list_feedername\n",
    "    df_APSA['complete']=list_finished\n",
    "    df_APSA['in_progress']=list_operation\n",
    "    df_APSA['total']=df_APSA['in_progress']+df_APSA['complete']\n",
    "\n",
    "    if area == 1:\n",
    "        list_feeder=List_N1_CB\n",
    "    elif area ==2:\n",
    "         list_feeder=List_N2_CB\n",
    "    elif area ==3:    \n",
    "        list_feeder=List_N3_CB\n",
    "\n",
    "    df2_APSA=pd.DataFrame()\n",
    "    for feeder in list_feeder:\n",
    "        if feeder in set(df_APSA['feedername']):\n",
    "            df2_APSA=df2_APSA.append(df_APSA.loc[df_APSA['feedername'].values==feeder])\n",
    "        else:\n",
    "            df_empty=pd.DataFrame()\n",
    "            df_empty['feedername']=[feeder]\n",
    "            df_empty['complete']=[0]\n",
    "            df_empty['in_progress']=[0]\n",
    "            df_empty['total']=[0]\n",
    "            df2_APSA=df2_APSA.append(df_empty)\n",
    "\n",
    "    df2_APSA=df2_APSA.reset_index()\n",
    "    df2_APSA.to_csv('APSA_DataFrame_{}.csv'.format(area), sep=',')\n",
    "    \n",
    "    df2_APSA=pd.read_csv('APSA_DataFrame_{}.csv'.format(area), sep=',')\n",
    "    df2_APSA['%in_complete']=1-df2_APSA['complete']/df2_APSA['total']\n",
    "    df2_APSA=df2_APSA.fillna(0)\n",
    "    # df_e_counter=pd.read_csv('e_counter_DataFrame_{}.csv'.format(area), sep=',')\n",
    "    df_equipment=pd.read_csv('equipment_{}.csv'.format(area), sep=',')\n",
    "    df_equipment= df_equipment.dropna()\n",
    "    df_equipment = df_equipment.drop_duplicates(subset='equipment')\n",
    "    df_equipment=df_equipment.reset_index()\n",
    "    df_equipment=df_equipment.drop(df_equipment.index[[ind for ind in df_equipment.index if 'F-' in df_equipment.iloc[ind,3]]])\n",
    "    df_equipment=df_equipment.reset_index()\n",
    "\n",
    "    del df_equipment['level_0']\n",
    "    del df_equipment['index']\n",
    "    del df_equipment['Unnamed: 0']\n",
    "\n",
    "\n",
    "    print('-------------retrieve OMS data of area {}-------------'.format(area))\n",
    "    if area == 1:\n",
    "        list_feeder_name=List_N1_feeder_name\n",
    "    elif area ==2:\n",
    "         list_feeder_name=List_N2_feeder_name\n",
    "    elif area ==3:    \n",
    "        list_feeder_name=List_N3_feeder_name\n",
    "    dict_feeder = {item1:{item2:[df_equipment.iloc[item3, 1] for item3 in set(np.where(df_equipment['type']==item2)[0]).intersection(np.where(df_equipment['feeder']==item1)[0])] for item2 in df_equipment['type'].unique()} for item1 in list_feeder_name}\n",
    "\n",
    "    if area == 1:\n",
    "        list_feeder_name=List_N1_feeder_name\n",
    "    elif area ==2:\n",
    "         list_feeder_name=List_N2_feeder_name\n",
    "    elif area ==3:    \n",
    "        list_feeder_name=List_N3_feeder_name\n",
    "\n",
    "    if area ==1:\n",
    "        df_counter_N1_1=pd.read_excel('OMS_N1_1.xlsx', index_col=None)\n",
    "        df_counter_N1_2=pd.read_excel('OMS_N1_2.xlsx', index_col=None)\n",
    "        dict_counter = get_TL_TR_values(df_counter_N1_1,df_counter_N1_2, list_feeder_name)\n",
    "\n",
    "    elif area ==2:\n",
    "        df_counter_N2_1=pd.read_excel('OMS_N2_1.xlsx', index_col=None)\n",
    "        df_counter_N2_2=pd.read_excel('OMS_N2_2.xlsx', index_col=None)\n",
    "        dict_counter = get_TL_TR_values(df_counter_N2_1,df_counter_N2_2, list_feeder_name)\n",
    "\n",
    "    elif area ==3:\n",
    "        df_counter_N3_1=pd.read_excel('OMS_N3_1.xlsx', index_col=None)\n",
    "        df_counter_N3_2=pd.read_excel('OMS_N3_2.xlsx', index_col=None)\n",
    "        dict_counter = get_TL_TR_values(df_counter_N3_1,df_counter_N3_2, list_feeder_name)\n",
    "    \n",
    "    \n",
    "#     #---------------------------------------------------------------#\n",
    "\n",
    "#     df_Nov = pd.read_csv('LOAD_SCADAN1_NOV.tsv', sep='\\t')\n",
    "#     del df_Nov['DBTM']\n",
    "#     del df_Nov['AVNR']\n",
    "#     del df_Nov['WISO']\n",
    "#     del df_Nov['ST_MIT']\n",
    "#     del df_Nov['ST_MAX']\n",
    "#     del df_Nov['DATUM_MAX']\n",
    "#     del df_Nov['WISO_MAX']\n",
    "#     del df_Nov['ST_MIN']\n",
    "#     del df_Nov['DATUM_MIN']\n",
    "#     del df_Nov['WISO_MIN']\n",
    "#     del df_Nov['ST_EFF']\n",
    "#     del df_Nov['TXNR']\n",
    "#     del df_Nov['DIM']\n",
    "\n",
    "#     df_Nov['Temp1'], df_Nov['Temp2'], df_Nov['Temp3'], df_Nov['Temp4']= df_Nov['BMTEXT'].str.split(':', 4).str\n",
    "#     df_Nov['Sub'], df_Nov['VL'], df_Nov['FDR']= df_Nov['Temp2'].str.split(' ', 2).str\n",
    "#     df_Nov['OUT'], df_Nov['MV']=df_Nov['Temp3'].str.split(' ', 1).str\n",
    "#     del df_Nov['BMTEXT']\n",
    "#     del df_Nov['Temp1']\n",
    "#     del df_Nov['Temp2']\n",
    "#     del df_Nov['Temp3']\n",
    "#     del df_Nov['Temp4']\n",
    "#     del df_Nov['VL']\n",
    "#     del df_Nov['FDR']\n",
    "#     del df_Nov['MV']\n",
    "\n",
    "#     df_Nov['OUT']=[clean_feeder(item) for item in df_Nov['OUT'].values]\n",
    "#     df_Nov['feeder']=[x+y[3:] for x,y in zip(df_Nov['Sub'],df_Nov['OUT'])]\n",
    "#     del df_Nov['Sub']\n",
    "#     del df_Nov['OUT']\n",
    "\n",
    "#     # print(df_Nov.head())\n",
    "\n",
    "#     load_N1={feeder:{'max':get_max_avarage_load(df_Nov, feeder)[0], 'mean':get_max_avarage_load(df_Nov, feeder)[1]} for feeder in List_N1_feeder_name}\n",
    "    \n",
    "#     #---------------------------------------------------------------#\n",
    "    \n",
    "    print('-------------retrieve max, average load data of area {}-------------'.format(area))\n",
    "    if area == 1:\n",
    "        list_feeder_name=List_N1_feeder_name\n",
    "    elif area ==2:\n",
    "         list_feeder_name=List_N2_feeder_name\n",
    "    elif area ==3:    \n",
    "        list_feeder_name=List_N3_feeder_name\n",
    "\n",
    "    df_input_layer=pd.DataFrame()\n",
    "    for feeder in list_feeder_name:\n",
    "        print(feeder)\n",
    "        temp_df_input_layer=pd.DataFrame()\n",
    "    #     input_layer1_1=df_e_counter.loc[df_e_counter['feeder'].values==feeder]\n",
    "        input_layer1_2=df2_APSA.loc[df2_APSA['feedername'].values==feeder+'VB01']\n",
    "        temp_df_input_layer['feeder']=[feeder]\n",
    "        temp_df_input_layer['sum_TR']=dict_counter[feeder]['T/R']\n",
    "        temp_df_input_layer['sum_TL']=dict_counter[feeder]['T/L']\n",
    "        temp_df_input_layer['in_progress']=input_layer1_2['%in_complete'].values[0]\n",
    "\n",
    "        if area == 1:\n",
    "#             temp_df_input_layer['Peak load']=load_N1[feeder]['max']\n",
    "#             temp_df_input_layer['Average load']=load_N1[feeder]['mean']\n",
    "            temp_df_input_layer['Peak load']=abs(get_max_load_N1(area, feeder, year, month))\n",
    "            temp_df_input_layer['Average load']=abs(get_ava_load_N1(area, feeder, year, month))\n",
    "        elif area == 2:\n",
    "            temp_df_input_layer['Peak load']=abs(get_max_load_N2(area, feeder, year, month))\n",
    "            temp_df_input_layer['Average load']=abs(get_ava_load_N2(area, feeder, year, month))\n",
    "        elif area == 3:\n",
    "            temp_df_input_layer['Peak load']=abs(get_peak_load_N3(area, feeder, year, month))\n",
    "            temp_df_input_layer['Average load']=abs(get_peak_load_N3(area, feeder, year, month)/4)\n",
    "\n",
    "        df_input_layer=df_input_layer.append(temp_df_input_layer)\n",
    "    df_input_layer=df_input_layer.reset_index()\n",
    "    del df_input_layer['index']\n",
    "    df_nor_input_layer=nor_input_layer(df_input_layer.copy())\n",
    "    # print(df_input_layer)\n",
    "    # print(df_nor_input_layer)\n",
    "    df_input_layer=df_input_layer.fillna(0)\n",
    "    df_nor_input_layer=df_nor_input_layer.fillna(0)\n",
    "          \n",
    "        \n",
    "        \n",
    "    print('-------------FHM calculation of area {}-------------'.format(area))\n",
    "    df_input_weight=pd.read_csv('weight_input.csv')\n",
    "    del df_input_weight['Load Break']\n",
    "    del df_input_weight['Recloser']\n",
    "    del df_input_weight['SCB']\n",
    "    del df_input_weight['Fixed Capacitor Bank']\n",
    "    del df_input_weight['LRR(FRTU)']\n",
    "    del df_input_weight['AVR']\n",
    "    list_input=['sum_TR','sum_TL','in_progress','Peak load','Average load']\n",
    "\n",
    "    input_range=[[0,1]]*(len(list(df_nor_input_layer))-1)  #[[x1_min, x1_max],[x2_min, x2_max],...,[xn_min, xn_max]]\n",
    "    number_input=len(input_range)\n",
    "    dict_FHI={}\n",
    "\n",
    "    if area == 1:\n",
    "        list_feeder_name=List_N1_feeder_name\n",
    "    elif area ==2:\n",
    "         list_feeder_name=List_N2_feeder_name\n",
    "    elif area ==3:    \n",
    "        list_feeder_name=List_N3_feeder_name\n",
    "\n",
    "    for feeder in list_feeder_name:\n",
    "    #     print(feeder)\n",
    "        num_equipment = len(dict_feeder[feeder]['CB'])+len(dict_feeder[feeder]['SCB'])+len(dict_feeder[feeder]['Recloser'])+len(dict_feeder[feeder]['Switch'])\n",
    "        num_CB=len(dict_feeder[feeder]['CB'])\n",
    "        num_Switch=len(dict_feeder[feeder]['Switch'])\n",
    "        num_Recloser=len(dict_feeder[feeder]['Recloser'])\n",
    "        num_SCB=len(dict_feeder[feeder]['SCB'])\n",
    "        if num_equipment !=0:\n",
    "            list_equipment=[1]*num_CB + [2]*num_Switch + [3]*num_Recloser + [4]*num_SCB\n",
    "            list_equipment_2=['w1']*num_CB + ['w2']*num_Switch + ['w4']*num_Recloser + ['w7']*num_SCB\n",
    "            input_weight=np.array([[float(df_input_weight.iloc[np.where(df_input_weight['Input/Equipment']==input1)[0],equipment])for input1 in list_input] for equipment in list_equipment])\n",
    "            nor_input_weight=np.array([input_weight[inp]/input_weight.sum(axis=1)[inp] for inp in np.arange(0,len(input_weight),1)])\n",
    "            output_weight=np.array([dict_nor_weights[equipment]/{'w1':num_CB, 'w2':num_Switch, 'w4':num_Recloser, 'w7':num_SCB}[equipment] for equipment in list_equipment_2])\n",
    "            nor_output_weight=output_weight/output_weight.sum(axis=0)\n",
    "            net1 = nl.net.newff(input_range, [num_equipment, 1], [nl.trans.PureLin(), nl.trans.PureLin()])  #num_equipment =len [CB1, CB2, REC1, REC2, SW1, SW2]\n",
    "\n",
    "            for input in np.arange(0,len(nor_input_weight),1):\n",
    "                net1.layers[0].np['w'][input]= nor_input_weight[input]\n",
    "            net1.layers[0].np['b']=[0]*num_equipment\n",
    "            net1.layers[1].np['w'][0]=nor_output_weight\n",
    "            net1.layers[0].np['b']=[0]*num_equipment\n",
    "            net1.layers[1].np['b']=[0]\n",
    "            df_nor_input_layer.iloc[np.where(df_nor_input_layer['feeder']==feeder)[0],1:]\n",
    "            output=net1.sim(np.array(df_nor_input_layer.iloc[np.where(df_nor_input_layer['feeder']==feeder)[0],1:]).tolist())\n",
    "    #         print(output)\n",
    "            dict_FHI.update({feeder:1-output[0][0]})\n",
    "        else:\n",
    "            dict_FHI.update({feeder:1})\n",
    "\n",
    "\n",
    "    # print(dict_FHI)\n",
    "    df_input_layer['FHI']=[dict_FHI[feeder] for feeder in df_input_layer['feeder'].tolist()]\n",
    "    print(df_input_layer)\n",
    "\n",
    "    with open(\"FHM_{}.json\".format(are),'w') as outfile:\n",
    "        json.dump(dict_FHI, outfile, cls=NumpyEncoder)\n",
    "    \n",
    "    print('-------------FHM area {} is saved-------------'.format(area))\n",
    "\n",
    "\n",
    "print('-------------add urban area -------------')\n",
    "\n",
    "\n",
    "#read all 3N json file\n",
    "list_FHI=[]\n",
    "\n",
    "list_urban={'1':['CMA01','CMA02','CMA03','CMA04','CMA05','CMA06','CMA07','CMA08','CMA09','CMA10','CMB01','CMB02','CMB03','CMB04','CMB05','CMB06','CMB07','CMB08','CMB09','CMB10','CMC01','CMC02','CMC03','CMD03','CMD07','CMD08','CMD10','CMF01','CMF02','CMF03','CMF04','CMF05','CMU01','CMU02','CMU03','CMU04','CMU05','CMV01','CMV02','CMV03','CMV04','CMV05','MRM07'],'2':['PLA02',\n",
    "'PLA03','PLA04','PLA05','PLA06','PLA07','PLA08','PLC01','PLC02','PLC03','PLC04','PLC06','PLC07','PLT05','WGT05','MSA04','MSA05','MSA06','MSA11','MSA12'],'3':['LBA01','LBA02','LBA03','LBA04','LBA05','LBA06','LBA07','LBA08','LBC01','LBC02','LBC03','LBC04','LBC05']}\n",
    "for area in [1,2,3]:\n",
    "    with open('FHM_{}.json'.format(area)) as json_data:\n",
    "        d=json.load(json_data)\n",
    "        if area == 1:\n",
    "            list_feeder_name=List_N1_feeder_name\n",
    "        elif area ==2:\n",
    "             list_feeder_name=List_N2_feeder_name\n",
    "        elif area ==3:  \n",
    "            print(d)\n",
    "            list_feeder_name=List_N3_feeder_name\n",
    "        list_FHI.append([{'Region':'1{}'.format(area), 'sub':feeder[0:3], 'feeder':feeder, 'HI':d[feeder], 'Urban':feeder in list_urban['{}'.format(area)]} for feeder in list_feeder_name])\n",
    "        \n",
    "all_dict_FHM={'raw_data':[item2 for item1 in list_FHI for item2 in item1]}\n",
    "\n",
    "with open(\"FHM.json\",'w') as outfile:\n",
    "    json.dump(all_dict_FHM, outfile, cls=NumpyEncoder)\n",
    "\n",
    "          \n",
    "print('-------------FHM.json file is saved -------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
